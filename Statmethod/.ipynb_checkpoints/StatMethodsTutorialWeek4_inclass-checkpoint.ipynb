{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Does seaborn exist on the computer? True\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.stats\n",
    "import scipy.optimize\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# iPython magic command for inline figures. If this command is not given, figures are generated in windows.\n",
    "# An asterisk appears in the cell prompt (i.e. \"In [*]:\"), and the kernel stalls until the window is closed.\n",
    "%matplotlib inline\n",
    "\n",
    "## Below are two simple examples of exception handling\n",
    "\n",
    "## Try to import pandas. If not existent, set flag = False, and raise an exception.\n",
    "try:\n",
    "    import pandas as pd\n",
    "    pd_flag = True\n",
    "except ImportError:\n",
    "    print(\"Package pandas not found. Cannot do funky data structures!\")\n",
    "    pd_flag = False\n",
    "    \n",
    "## Try to import seaborn.\n",
    "try:\n",
    "    import seaborn as sns\n",
    "    sns_flag = True\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    sns.set_style(\"ticks\")\n",
    "except ImportError:\n",
    "    sns_flag = False\n",
    "print(\"Does seaborn exist on the computer? \" + str(sns_flag))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Significance tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many sigmas?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is quite common, especially in experimental physics, to quote significance (or confidence levels/intervals, see later) in terms of a number of *sigmas*. The sigma value ($\\sigma$) corresponding to a given $p$-value is simply the number of standard deviations of a standard normal distirbution which exclude an integrated probability equal to the $p$-value. I.e. $p$-value = Pr$\\left(\\left|x\\right|\\ge{z}\\mid\\mu = 0, \\sigma^2=1\\right)$ (remembering that the probability is calculated on both sides of the distribution, hence the modulus for the $x$). A sigma value for a given $p$-value or vice versa can be quickly calculated using the standard normal distribution in `scipy.stats`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3-sigma corresponds to p = 0.00269979606326\n",
      "5-sigma corresponds to p = 5.73303143758e-07\n",
      "p-value of 0.05 corresponds to 1.95996398454 sigma\n",
      "p-value of 0.01 corresponds to 2.57582930355 sigma\n"
     ]
    }
   ],
   "source": [
    "def sigmas_to_p(sigval):\n",
    "    nd = scipy.stats.norm(0., 1.)\n",
    "    return 2.*nd.sf(sigval)\n",
    "\n",
    "def p_to_sigmas(pval):\n",
    "    nd = scipy.stats.norm(0., 1.)\n",
    "# We use the 'inverse survival function', the inverse of the cdf. We also need to divide\n",
    "# our p-value by 2 to account for the negative side of the normal distribution.\n",
    "    return nd.isf(pval/2.)\n",
    "\n",
    "print \"3-sigma corresponds to p =\",sigmas_to_p(3.)\n",
    "print \"5-sigma corresponds to p =\",sigmas_to_p(5.)\n",
    "\n",
    "print \"p-value of 0.05 corresponds to\",p_to_sigmas(0.05),\"sigma\"\n",
    "print \"p-value of 0.01 corresponds to\",p_to_sigmas(0.01),\"sigma\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The t-test: revisiting Michelson's speed-of-light data\n",
    "\n",
    "Let's recycle and hack some of the code we used from Week 1 (originally to split up the data to calculate statistical quantities separately for each experiment).  Using the known speed of light (299000+792.5 km/s) for our population mean, we obtain t-statistics and p-values for the 1-sample t-statistic as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "morley = np.loadtxt(\"morley.txt\", skiprows=1)\n",
    "run = morley[:,1]\n",
    "speed = morley[:,2]\n",
    "experiment = morley[:,3]\n",
    "\n",
    "## Loop over all experiments and print and write t-statistic and p-value to arrays\n",
    "tstat_all = []\n",
    "pval_all = []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collectively, it is clear that these experiments give values for the mean which are systematically too high compared to the expected value.  Note that if the t-statistics were negative, that would imply values of the mean which were <i>lower</i> than the expected value.  For the t-statistic, the absolute value conveys the size of the deviation, but the sign tells us the direction, thus the t-test is a <i>two-sided test</i>.  To see what this means in terms of the <i>p</i>-value, let's look at the result from the 4th experiment (with the least significance).\n",
    "\n",
    "We can plot the obtained t-statistic (2.0855...) on the corresponding t-distribution, which is for 19 degrees of freedom (d.o.f.), since the d.o.f. $\\nu=n-1$ where $n$ is the number of measurements used to obtain the mean, in this case $n=20$ (i.e. per experiment):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-740c309883fd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m4.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0max1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpdf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxvline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtstat_all\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mymin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mymax\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlinewidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlinestyle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'dotted'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'red'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0max1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_xlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"t\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfontsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0max1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_ylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"probability density\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfontsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAl4AAAF3CAYAAAB0akXlAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\niHTI/wAZYPhwDoAoiwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1099d5310>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## First freeze the distribution for the given degrees of freedom (which is the only 'shape parameter' of the \n",
    "## distribution)\n",
    "t = scipy.stats.t(df=19)\n",
    "\n",
    "## Now lets plot the pdf, together with a vertical line marking the measured value of the t-statistic:\n",
    "fig, ax1 = plt.subplots(1, figsize=(10,6))\n",
    "x = np.arange(-4.0, 4.0, 0.1)\n",
    "ax1.plot(x, t.pdf(x), lw=2)\n",
    "plt.axvline(x=tstat_all[3], ymin=0.0, ymax = 1.0, linewidth=2, linestyle='dotted', color='red')\n",
    "ax1.set_xlabel(\"t\", fontsize=20)\n",
    "ax1.set_ylabel(\"probability density\", fontsize=20)\n",
    "ax1.tick_params(axis='x', labelsize=20)\n",
    "ax1.tick_params(axis='y', labelsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our significance test we need to properly frame the question in terms of the null hypothesis.  What is the null hypothesis and the corresponding question we must answer in this case?\n",
    "\n",
    "<b>\"Assuming that the true (population) mean is 299792.5 km/s, what is the probability that we would see such an extreme (or more extreme) value of the <i>t</i>-statistic from the data (assuming that the errors are normally distributed)?\"</b>\n",
    "\n",
    "What do we mean by \"such an extreme value\"?  We are talking about how far the sample mean is from the assumed population mean (normalised by the standard error), thus larger <i>absolute</i> values of the t-statistic (either negative or positive) are classed as more extreme (i.e. the t-test is a 2-sided test).  Since we would be even more surprised to obtain even larger absolute values of $t$ than we observe <i>if the assumed population mean is correct</i>, we want to know the probability that $|t|\\ge |t_{\\rm observed}|$.  This is our p-value.  Thus for the 4-th experiment we can recover the $p$-value by integrating the $t$-statistic over these extrema, using the appropriate suffix on our function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which can be compared with the value already given by the 'scipy.stats.t' function.\n",
    "\n",
    "How should we report the significance of our results?  It depends on the context: how big a deal is it if the null hypothesis is wrong?  If you are trying to test whether the speed-of-light is really variable, it is unlikely that anybody would accept anything less than 5-sigma ($p\\sim 6\\times 10^{-7}$ or almost 1 in 2 million), and even then would suspect a systematic error in the experiment as the likely origin of the deviation (the process would then move on to replication by other groups and with different experimental set-ups).  \n",
    "\n",
    "More likely, we are curious about more mundane effects, e.g. are we looking to check something which is already suspected or known or is fairly plausible versus our null hypothesis in the light of other evidence? In this case, given we know how much evidence there is for the speed-of-light being constant (this is our 'inner Bayesian' talking, even if we don't know it!) we may suspect that there a systematic error in the experimental measurements, and would accept much lower significances as evidence for this.  Typically then, we look at what is the nearest appropriate significance limit <i>below</i> the obtained value.  The limits can then be quoted as a sigma value, or in terms of a percentage given by $1-p$ (which is our <b><i>confidence</i></b> that the null hypothesis is disproven).  \n",
    "\n",
    "A bare minimum for claiming significance is either the 95~per cent confidence level (common to many fields and seen as a potential cause of high publication of false results), or more robustly, 3-sigma (99.7 per cent confidence).\n",
    "\n",
    "So, looking at each experiment above, what is the significance of each deviation from the known speed of light?\n",
    "\n",
    "Experiment 1:  \n",
    "\n",
    "Experiment 2:  \n",
    "\n",
    "Experiment 3:  \n",
    "\n",
    "Experiment 4:  \n",
    "\n",
    "Experiment 5:  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The $\\chi^{2}$ test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we examine the chi-squared test.  Here we must assume an underlying model which describes the expected value ($y_{\\rm mod}$) of each data point (at a value of the explanatory variable, $x$).  Assuming that the model and its parameters are correct and the errors on the data points are independent and normally distributed, the sum of the squared, weighted (by the error) residuals will follow a $\\chi^{2}$ distribution with $\\nu$ degrees of freedom.  $\\nu = n-m$ where $n$ is the number of data points and $m$ the number of <i>free</i> (i.e. allowed to vary) parameters in the model. \n",
    "\n",
    "First let's define a function for calculating the summed weighted least-squares.  We will do this in a general way so we can use it again later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def ChiSq(parm, xval, yval, dy): # the weighted least-squares\n",
    "    ymod = my_model(xval, parm) # model to be specified by a different function\n",
    "    return sum(pow((yval-ymod)/dy,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To demonstrate the test, we will assume the null hypothesis that the values of the speed of light measured by Michelson's experiments are consistent with a constant value equal to the mean of the five measurements.  Thus we are effectively checking the idea that the measurements are consistent with one another and the <i>observed</i> mean, i.e. there is no systematic error between experiments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##  First we calculate the mean and standard error for each experiment\n",
    "yval = []\n",
    "yerr = []\n",
    "for s in set(experiment):\n",
    "    mean = np.mean([sp for sp, ex in zip(speed, experiment) if ex == s])\n",
    "    err = scipy.stats.sem([sp for sp, ex in zip(speed, experiment) if ex == s])\n",
    "    yval.append(mean)\n",
    "    yerr.append(err)\n",
    "\n",
    "##  Now define a 'model' by calculating the mean - parm is just used to substitute the y values here\n",
    "parm = yval\n",
    "def my_model(xval, parm):\n",
    "    return np.mean(parm)\n",
    "\n",
    "##  Now determine and print chi-squared:\n",
    "xsq_obs = \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many degrees of freedom are there?  We have $n=5$ data points.  At face value, the model has <i>no</i> free parameters - the model is given by the mean which is a fixed value.  However, it is fixed by the data, equivalent in some sense to a best-fit and so we would be right to assume that $m=1$, thus there are 4 degrees of freedom.  We can now plot our measured value on the $\\chi^{2}_{4}$ pdf:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Plot the pdf and measured value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can already see from the figure that our observed value is well in the tail of the distribution.  Note that the chi-squared test is a 1-sided test: smaller values lie closer to the null hypothesis not further away, so if we want to test whether the data are consistent with the null-hypothesis (the assumed model), we should ask what is the probability that an equal or larger value of chi-squared would be observed by chance, if the model is correct:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Print the p-value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus our confidence that the null hypothesis is false (i.e. the experimental results are in fact <b>not</b> consistent between experiments) is greater than 99 per cent, but still just less than 3-sigma.\n",
    "\n",
    "It's interesting to consider the case where our observed chi-squared was significantly small, compared to the number of degrees of freedom.  E.g. if the observed chi-squared was 0.2 in the above example, we would be in the far left tail of the distribution, supporting the null hypothesis but also unlikely ($<1$ per cent) to occur by chance.  Of course, low-probability events do happen (especially at modest but not extreme probabilities), but we might still start to suspect that the error bars are somehow overestimated, and look into this possibility."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximum likelihood estimation\n",
    "\n",
    "Maximum likelihood estimation is the process of fitting models to data so that we can find the best-fitting model parameters (the <i>Maximum Likelihood Estimates</i> or <i>MLEs</i>), along with (potentially) an estimate of the goodness-of-fit of the model.  The methods can also be extended to provide confidence intervals ('error bars') on our model parameters, and also to compare different (but related) models to determine which provides the best description of the data (hypothesis testing).  We will investigate these aspects of maximum likelihood estimation in the next couple of weeks, but first we examine how to use likelihood functions combined with the suite of powerful optimisation tools in Python, in order to fit models to data and estimate the goodness-of-fit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Likelihood functions: Log-likelihood and weighted least-squares\n",
    "\n",
    "Before proceeding to fitting we must first define our likelihood function. Here we follow Vaughan and define a couple of possible functions: the negative log-likelihood and the weighted least-squares (i.e. to get the $\\chi^{2}$ statistic). In both cases we need to minimise the function to find the MLEs. You can choose your own form of the likelihood function to suit the data you need to fit. The negative log-likelihood calls our model and then sets up the normal distribution with a mean given by the model prediction for $y$ and a standard deviation equal to the assumed error, $dy$. We can then obtain the likelihoods from the pdf of our normal distribution. This form of the likelihood function is very flexible, since instead of a normal distribution we can choose other distributions that may be better suited to our data, i.e. a more appropriate 'statistical model' (most likely the Poisson distribution, which we will look at next week).   \n",
    "\n",
    "We have already seen the weighted least-squares approach in the form of the significance test: this function is essentially the same as the one used for the test (the underlying statistical principles are the same, and this will give us the ability to assign a confidence or 'goodness-of-fit' to our best-fitting model). This is a much simpler likelihood function, but remember that it is only applicable in the case where the errors are normally distributed!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def LogLikelihood(parm, xval, yval, dy): # the -ve log-likelihood\n",
    "    ymod = my_model(xval, parm)  #We define our 'physical model' separately\n",
    "    nd = scipy.stats.norm(ymod, dy) #we define our normal distribution\n",
    "    return -sum(np.log(nd.pdf(yval)))\n",
    "\n",
    "def ChiSq(parm, xval, yval, dy): # the weighted least-squares\n",
    "    ymod = my_model(xval, parm)\n",
    "    return sum(pow((yval-ymod)/dy,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting a constant to the speed of light data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll first look at a very simple example of fitting a model to data, specifically, we are going to find the constant value which best-fits the combined means (and errors) of the five Michelson experiments.  First define the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "parm = np.zeros(1)  ## First set parm to be a 1-element array - we will set the value later\n",
    "def my_model(xval, parm):\n",
    "    return parm[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, our model is very simple and you could easily find a Python function to do the same job, but being able to define your own models allows you to do ML estimation for much more complex data.  \n",
    "\n",
    "Since we have set up our estimator - the log-likelihood or weighted least-squares - for our model, we can minimise it to find the best fitting parameters. For this we use the `scipy.optimize` module, specifically the minimize function. Besides giving this function the name of our likelihood function and the parameters to be varied to find their MLEs, we also provide the other variables used by the likelihood function (i.e. the data!) in the `args` parameter. \n",
    "\n",
    "As described in the Tutorial lecture slides, there are many options to choose from.  For simple minimisation where we aren't interested in outputting the Jacobian or Hessian (useful for estimating confidence intervals: see next week), we can use one of the standard functions like Nelder-Mead with `minimize`.\n",
    "Nelder-Mead also does not require a Jacobian to be specified for our physical model function (the gradient information in the Jacobian is used by some minimisation methods to help find the minimum). There are many more options which can be found in the documentation of the `scipy.optimize` module and associated documentation for the `minimize` function. The question of how best to do minimisation is complex and it very much depends on the type of data, models and your own computational resources, so it is best to read further on the topic if you have a specific problem to solve.  We will look at some further examples \n",
    "\n",
    "Now we'll import the `scipy.optimize` package to make the function calls simpler.  Then we will set up our starting parameters and call the optimisation function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import scipy.optimize as op\n",
    "\n",
    "## First set the starting values of model parameters.  It depends on the model function used, \n",
    "## but in general you should make sure these aren’t way off the expected values or the optimisation may get stuck\n",
    "## - this doesn’t mean you need to know the values already, just choose something\n",
    "## that seems plausible given the data.\n",
    "\n",
    "parm[0] = 800.\n",
    "## We can recycle the means and standard errors calculated earlier in the notebook, for clarity we also\n",
    "## set the xval to the experiment number, but note in this case that the model is not a function of this value\n",
    "## (in most other situations, the model for y will depend on x)\n",
    "xval = [1, 2, 3, 4, 5]\n",
    "\n",
    "## Now the optimisation function: we can also change the function called to ChiSq\n",
    "## without changing anything else, since the parameters are the same.\n",
    "\n",
    "result = op.minimize(LogLikelihood, parm, args=(xval, yval, yerr), method=\"Nelder-Mead\")\n",
    "\n",
    "## The minimize function outputs the array of best-fitting parameters (x) and the value of the function minimised \n",
    "## e.g. the -ve Log-likelihood, or the chi-squared.\n",
    "\n",
    "mle = result[\"x\"]\n",
    "mlval = result[\"fun\"]\n",
    "print \"-ve log-likelihood fit results:\"\n",
    "print \"Best-fitting constant = \" + str(mle[0]) + \" with -ve log-likelihood = \" + str(mlval)\n",
    "\n",
    "## Changing to weighted-least squares...\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's interesting to note that the best-fitting constant to the data here is significantly different from the mean of all the data (852.4) calculated previously.  This is because it has been weighted by the standard error on the mean for each experiment, with the experiments with greater scatter in their recorded values contributing less to the model fit.  As a result of this, the chi-squared value is also significantly smaller than the value we obtained earlier for the unweighted mean.  \n",
    "\n",
    "With the output of the weighted least-squares fit, we can go further and estimate the significance using the $\\chi^{2}$ distribution for 4 d.o.f. (5 data points - 1 free parameter in the model).  This is our <i>goodness-of-fit</i>.  Calculate it using the function you used earlier - clearly the obtained constant is a much better estimator of the underlying mean of the experiment, and the goodness-of-fit implies that the experiments are in fact consistent with a single mean value (although one which is systematically too high compared to the true value of the speed-of-light).\n",
    "\n",
    "You can feel free to experiment with using other methods for fitting beyond Nelder-Mead, but note that in this case, for the very simple function minimisation as used here, the choice of method probably won't have much impact on the fitting time, which will be effectively instantaneous."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting Reynolds' data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now build some code to find the MLEs for the parameters of the simple linear model that can be used to fit the laminar-flow regime of Reynolds’ data. First we load in the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reynolds = np.genfromtxt (\"reynolds.txt\", dtype=np.float, names=[\"dP\", \"v\"], skip_header=1, autostrip=True)\n",
    "## change units\n",
    "ppm = 9.80665e3\n",
    "dp = reynolds[\"dP\"]*ppm\n",
    "v = reynolds[\"v\"]\n",
    "## Now select the first 8 pairs of values, where the flow is laminar,\n",
    "## and assign to x and y\n",
    "xval = dp[0:8]\n",
    "yval = v[0:8]\n",
    "dy = 6.3e-3  ## We will assume a constant error for the purposes of this exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to define our <i>physical</i> model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Here, parm is the vector of parameters, intercept a (parm[0]) and gradient b (parm[1])\n",
    "# Now define the model:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our <i>statistical</i> model we will assume that the errors are normally distributed with the value given for the standard deviation, and thus we can use the weighted least-squares likelihood function (which allows us to obtain the chi-squared value and hence also a goodness-of-fit).\n",
    "\n",
    "As above, the output of the `minimize` function is assigned to `result`, and different output variables can be\n",
    "provided depending on what is called, with further information provided under the `scipy.optimize.OptimizeResult` class description. `x` provides the MLEs of the `parm` model parameters, while `fun` provides the value of the likelihood function obtained by the minimisation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# First set the starting values of model parameters.  You should make sure\n",
    "# these aren’t way off the expected values or the optimisation may get stuck\n",
    "# - this doesn’t mean you need to know the values already, just choose something\n",
    "# that seems plausible given the data.\n",
    "\n",
    "# Now the optimisation function: we can also change the function called to ChiSq\n",
    "# without changing anything else, since the parameters are the same.\n",
    "\n",
    "\n",
    "# Output the results\n",
    "\n",
    "\n",
    "# Output the goodness-of-fit\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the fitting results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the returned parameter MLEs to make a graphical comparison with the data (which includes the error bars), plotting the data vs. the model, and also the residuals. These plots are useful to check that the model fit looks reasonable and allow us to check for any systematic structure in the residuals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1,2, figsize=(12,6)); plt.subplots_adjust(wspace=0.5)\n",
    "ax1.errorbar(xval, yval, yerr=dy, marker=\"o\", linestyle=\"\")\n",
    "ax1.plot(xval, my_model(xval,[a_mle, b_mle]), lw=2)\n",
    "ax1.set_xlabel(\"Pressure gradient (Pa/m)\", fontsize=20)\n",
    "ax1.set_ylabel(\"Velocity (m/s)\", fontsize=20)\n",
    "ax1.tick_params(labelsize=16)\n",
    "ax2.errorbar(xval, yval-my_model(xval,[a_mle, b_mle]), yerr=dy, marker=\"o\", linestyle=\"\")\n",
    "ax2.set_xlabel(\"Pressure gradient (Pa/m)\",fontsize=20)\n",
    "ax2.set_ylabel(\"Velocity residuals (m/s)\", fontsize=20)\n",
    "ax2.axhline(0.0, color='r', linestyle='dotted', lw=2) ## when showing residuals it is useful to also show the 0 line\n",
    "ax2.tick_params(labelsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no obvious systematic structure in the residuals and the fits look reasonable. Note that, when doing weighted least-squares fitting, the reduced chi-squared returned by dividing the `mlval` parameter by the degrees-of-freedom (6 in this case, since there are 8 data points and 2 free parameters) is close to 1. This also implies a good fit to the data, and that the error bars are not overestimated (you could try doubling the size of the error bars and repeating the procedure and the plots...)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapping out the likelihood using brute force and plotting the likelihood surface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can map out the way the likelihood changes with the model parameters using computational ‘brute force’, simply by stepping through a grid of the parameters and calculating the likelihood function for each point on the grid. This is computationally expensive but allows us to search the ‘likelihood surface’ quite effectively, to ensure our fit does not get stuck in any local minima. This is not really a concern with the model we are fitting here, but may be quite useful when fitting more complex models to data. The code below will also print the minimum value of negative log-likelihood, which you can compare with the value obtained from minimisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nvals_a = 50\n",
    "nvals_b = 50\n",
    "aval = np.linspace(-0.01, 0.03, nvals_a)\n",
    "bval = np.linspace(0.003, 0.004, nvals_b)\n",
    "ll = np.zeros((nvals_a,nvals_b))\n",
    "for i in range(nvals_a): ## x dimension\n",
    "    for j in range(nvals_b): ## y dimension\n",
    "        ll[i,j] = LogLikelihood([aval[i], bval[j]], xval, yval, dy)\n",
    "print \"Brute force minimum value\", np.amin(ll)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to make a plot of our likelihood surface. We can do this using matplotlib’s surface function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#  First we assign all three variables to their own 2D arrays:\n",
    "X, Y = np.meshgrid(aval,bval)\n",
    "Z = np.exp(-ll)\n",
    "#  Now call the surface function and import the 3d axes module to make some\n",
    "#  seriously funky plots!\n",
    "from mpl_toolkits.mplot3d import axes3d\n",
    "fig = plt.figure(figsize=(12,12))\n",
    "ax = fig.gca(projection=\"3d\")\n",
    "ax.plot_surface(X, Y, Z, rstride=1, cstride=1, alpha=0.3)\n",
    "ax.set_xlabel(\"a\",fontsize=20)\n",
    "ax.set_xlim(-0.01, 0.03)\n",
    "ax.set_ylabel(\"b\",fontsize=20)\n",
    "ax.set_ylim(0.003, 0.004)\n",
    "ax.tick_params(labelsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can even rotate and move around the non-inline version of the plot if you wish to explore the shape of the surface!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
